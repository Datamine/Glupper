#!/usr/bin/env python3
"""
ArchiveBox Worker Script

This script runs on the ArchiveBox server to process archive jobs from the SQS queue.
It handles the following steps:
1. Polls the SQS queue for new archive jobs
2. Archives the URL using ArchiveBox
3. Uploads the archive to S3 with a completion marker
4. No database interactions - S3 is the source of truth

Usage:
    python worker.py

Requirements:
    - AWS credentials configured
    - ArchiveBox installed and configured
"""

import json
import logging
import os
import sys
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import boto3
import httpx
from botocore.exceptions import ClientError

from config import (
    AWS_ACCESS_KEY_ID,
    AWS_REGION,
    AWS_SECRET_ACCESS_KEY,
    ARCHIVE_QUEUE_URL,
    POLL_INTERVAL,
    SQS_VISIBILITY_TIMEOUT,
)
from archivebox_util import archive_url_api, get_snapshot_files
from s3_util import (
    check_archive_exists,
    create_zip_archive,
    upload_completion_marker,
    upload_file_to_s3,
)

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    stream=sys.stdout,
)
logger = logging.getLogger("archivebox_worker")

# Initialize SQS client
sqs_client = boto3.client(
    "sqs",
    region_name=AWS_REGION,
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
)


def archive_and_upload(url: str, archive_id: str, title: Optional[str] = None) -> bool:
    """
    Archive a URL and upload the files to S3
    
    Args:
        url: The URL to archive
        archive_id: The archive ID (pre-generated by Glupper server)
        title: Optional title for the page
        
    Returns:
        True if successful, False otherwise
    """
    try:
        # First check if the archive already exists
        if check_archive_exists(archive_id):
            logger.info(f"Archive {archive_id} already exists, skipping")
            return True
            
        # Archive the URL using ArchiveBox
        logger.info(f"Archiving URL {url} with ID {archive_id}")
        archive_result = archive_url_api(url)
        
        if not archive_result:
            logger.error(f"Failed to archive URL {url}")
            return False
            
        # Extract snapshot ID
        snapshot_id = archive_result.get("id")
        if not snapshot_id:
            logger.error(f"Invalid archive result: {archive_result}")
            return False
            
        # Get the snapshot files
        snapshot_files = get_snapshot_files(snapshot_id)
        if not snapshot_files:
            logger.warning(f"No files found for snapshot {snapshot_id}")
            return False
            
        # Upload each file to S3
        logger.info(f"Uploading {len(snapshot_files)} files to S3")
        uploaded_files = []
        main_file = "index.html"  # Default main file
        
        for file_info in snapshot_files:
            file_path = file_info["path"]
            file_name = file_info["filename"]
            content_type = file_info["content_type"]
            
            # Main index.html file is the most important
            if file_name == "index.html":
                main_file = file_name
                
            # Upload the file
            s3_url = upload_file_to_s3(
                file_path=file_path,
                archive_id=archive_id,
                filename=file_name,
                content_type=content_type,
            )
            
            if s3_url:
                uploaded_files.append({
                    "filename": file_name,
                    "url": s3_url,
                    "content_type": content_type,
                })
        
        # Also create and upload a ZIP archive
        logger.info("Creating ZIP archive")
        zip_url = create_zip_archive(snapshot_files, archive_id)
        if zip_url:
            uploaded_files.append({
                "filename": "archive.zip",
                "url": zip_url,
                "content_type": "application/zip",
            })
        
        # Upload completion marker
        logger.info("Uploading completion marker")
        metadata = {
            "url": url,
            "title": title,
            "timestamp": datetime.now().isoformat(),
            "snapshot_id": snapshot_id,
            "files": uploaded_files,
            "main_file": main_file,
        }
        
        if upload_completion_marker(archive_id, metadata):
            logger.info(f"Successfully archived URL {url} with ID {archive_id}")
            return True
        else:
            logger.error(f"Failed to upload completion marker for {archive_id}")
            return False
            
    except Exception as e:
        logger.error(f"Error archiving URL {url}: {str(e)}")
        return False


def process_message(message):
    """Process a message from the SQS queue"""
    try:
        # Parse the message body
        body = json.loads(message["Body"])
        
        # Extract data
        url = body.get("url")
        archive_id = body.get("archive_id")
        title = body.get("title")
        
        if not url or not archive_id:
            logger.error(f"Invalid message format: {body}")
            return True  # Mark as processed to remove from queue
            
        logger.info(f"Processing archive job for URL {url} with ID {archive_id}")
        
        # Archive and upload
        success = archive_and_upload(url, archive_id, title)
        
        return success
    except Exception as e:
        logger.error(f"Error processing message: {e}")
        return False


def poll_queue():
    """Poll the SQS queue for new messages"""
    while True:
        try:
            # Receive message from SQS queue
            response = sqs_client.receive_message(
                QueueUrl=ARCHIVE_QUEUE_URL,
                MaxNumberOfMessages=1,
                WaitTimeSeconds=20,
                VisibilityTimeout=SQS_VISIBILITY_TIMEOUT,
                MessageAttributeNames=["All"],
            )
            
            messages = response.get("Messages", [])
            if not messages:
                # No messages, sleep before polling again
                time.sleep(POLL_INTERVAL)
                continue
                
            # Process each message
            for message in messages:
                receipt_handle = message["ReceiptHandle"]
                
                # Process the message
                success = process_message(message)
                
                # Delete the message from the queue if processed successfully
                if success:
                    sqs_client.delete_message(
                        QueueUrl=ARCHIVE_QUEUE_URL,
                        ReceiptHandle=receipt_handle,
                    )
                
        except KeyboardInterrupt:
            logger.info("Stopping worker")
            break
        except Exception as e:
            logger.error(f"Error polling queue: {e}")
            time.sleep(POLL_INTERVAL)


if __name__ == "__main__":
    logger.info("Starting ArchiveBox worker")
    poll_queue()